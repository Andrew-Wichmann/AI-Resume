# Event Bus

Tags: Architecture, Design, Support
Date: July 1, 2021 → February 1, 2024

# Executive Summary

- Create Python and GoLang libraries for building event driven applications
    - Released a major refactor and breaking change to the python API to vastly improve observability and simplify the API
- Create documentation for using the libraries and requesting cloud infrastructure for them
- Review and release new event types
- Brought in a new team to do maintenance
    - Presented history, current work, and future plans

# Details

In mid 2021, my team and I were requested by the head of architecture to support the development of event-driven architecture at Mintel. We were requested to use SQS and SNS to implement a fan out and router-less architecture. The motto we were given was, “smart endpoints and dumb pipes”. We were also instructed to use protobuf for message serialization for its forward-compatibility features. With these requirements, we were able to produce a series of GitlabCI pipelines to compile the protobufs into Python and GoLang bindings. With these bindings we made two classes of libraries: producer libraries and consumer libraries. Without an initial business use-case, we decided to test our deployment in our team’s AWS account using our own terraform. After a decision to port the cloud infrastructure to an infrastructure team-managed account, I worked side-by-side with that team to produce terraform modules to create the SQS and SNS resources. After we had the terraform modules working in production, I decided we needed to document the process by which teams requested new event bus resources. With the help of some proof-readers, I wrote step-by-step instructions in Confluence including what teams to request from and what support emails to contact. We configured the support email to raise issues in GitLab, and, from there, a message was sent to our team’s private chat application where we could discuss and assign a responder. At this point, we transitioned into a maintenance and support role for the infrastructure and client libraries. This primarily included approving new protobuf event definitions and sending terraform MRs to the infrastructure team for new SNS+SQS resources.

In mid 2022, the topic of distributed tracing caught my attention, and I longed to implement it in the event libraries. In addition to improved observability, I got feedback from an internal client of the library that the code seemed too complicated. The combination of those two inspired me to rewrite the consumer library with a much simpler API. Instead of using a thread pool to distribute message processing in python, I decided it was much simpler to have a synchronous application and deal with concurrency with a K8s autoscaling policy. The resulting API was a python context manager that blocked the main thread using SQS long polling to receive a message. In addition to a simpler API, the context manager registered a Prometheus client to measure several important events:

- how long processing of each event took
- how many messages succeeded, failed, or were discarded (client libraries whitelist events and all others are dropped)
- how long it took to receive a message from AWS SQS
- how many times the client library had to retry the message poll
- how many times SQS polling failed

Metrics are nice, but to super charge them, I implemented distributed tracing using OTEL and attached exemplar traces to the metrics. Lastly, as GrafanaLabs has taught me, metrics and traces are only two of the three pillars of observability, and, so, I also implemented structured logging and included the TraceID in each log to take advantage of Grafana Loki’s ability to navigate between logs and traces.

With all these improvements, I needed feedback and I needed to generate interest with the hopes that teams would welcome a major breaking change to their applications. Therefore, I decided to engage in an internal marketing campaign including private and public demos. I did two private demos: one to my immediate team and another to our first internal client. I also did a “public” demo with a full slide deck and live demonstration that included an example of events published to an SNS topic on my local machine to a locally running AWS localstack container, pulled from an SQS queue on that localstack container, processed by a “hello world” application whose traces were published to a locally running Grafana Tempo instance. I had a Grafana container with Tempo as a datasource to display the traces. In the next 6 months, we deployed three 2.0 consumers.

In late 2023, after an organizational restructuring, we were requested to include a new “core services” team in the support and maintenance of the event bus libraries. I organized created another slide deck, freshened up the support documentation, presented the stack to the new team, included them on support tickets, and administered access to all email, Gitlab, and Jira groups.